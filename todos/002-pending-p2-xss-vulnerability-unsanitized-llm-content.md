---
status: pending
priority: p2
issue_id: 002
tags:
  - code-review
  - security
  - xss
  - llm-integration
dependencies:
  - 001
---

# 002: XSS Vulnerability — Unsanitized LLM Content in Placeholder Options

## Problem Statement

The placeholder MCQ options embed untrusted LLM-generated content (the `correct_answer`) directly into f-strings without sanitization. This creates an XSS vulnerability if the LLM outputs malicious or unexpected content.

**Impact**: Student browsers could execute injected JavaScript if the LLM returns special characters (quotes, HTML, event handlers) in the correct answer.

**Attack Example**:
```
LLM generates: correct_answer = '5" onclick="alert(\'XSS\')"'
Placeholder becomes: 'A) 5" onclick="alert(\'XSS\')"'
When rendered in HTML: <span>A) 5" onclick="alert('XSS')"></span>
Result: JavaScript executes in student's browser
```

## Findings

### Location
File: `/Users/greggurevich/Library/CloudStorage/OneDrive-Personal/Python/personal/mora/services/question_service.py`
Lines: 262-270 (where LLM content is embedded)
Related: Route handlers that render questions to students

### Vulnerability Chain
```
LLM Output (untrusted)
  ↓
correct_answer field
  ↓
Embedded in f-string: f'A) {correct}'  [NO SANITIZATION]
  ↓
Stored in options array
  ↓
JSON serialized: json.dumps(options) at line 311
  ↓
Sent to frontend in HTTP response
  ↓
Rendered by JavaScript/Jinja2 template
  ↓
Potential XSS if template doesn't escape
```

### Current Mitigations (Partial)
1. **JSON encoding** (line 311): `json.dumps(q_data.get('options'))` — provides some escaping
2. **Jinja2 auto-escaping** (if enabled in Flask app) — would escape HTML entities
3. **Content-Security-Policy header** (if configured) — would block inline scripts

**Gap**: No sanitization at source. Relies on downstream layers to catch malicious content.

### Why This Matters
- LLMs can output unexpected content: typos, HTML entities, special characters
- Math questions might include `\sqrt`, `\(`, `\)` (LaTeX delimiters)
- User-uploaded content might be included in node descriptions (secondary injection)

## Proposed Solutions

### Solution A: Sanitize at Source (RECOMMENDED)
**Approach**: Add a sanitization function that escapes dangerous characters before embedding in options.

**Code**:
```python
def _sanitize_answer_text(text):
    """Escape special characters in answer text for safe embedding in options.

    Removes/escapes: HTML entities, quotes, newlines, control characters.
    Safe for embedding in f-strings and JSON serialization.
    """
    if not text:
        return ''

    # Convert to string (in case it's a number)
    text = str(text)

    # Remove control characters
    text = ''.join(c for c in text if ord(c) >= 32 or c in '\n\t')

    # Escape HTML special characters
    text = text.replace('&', '&amp;')
    text = text.replace('<', '&lt;')
    text = text.replace('>', '&gt;')
    text = text.replace('"', '&quot;')
    text = text.replace("'", '&#x27;')

    # Remove newlines (questions should be single line)
    text = text.replace('\n', ' ').replace('\r', ' ')

    return text

# Usage at line 262:
if q_type == 'mcq' and q_data and not q_data.get('options'):
    correct = _sanitize_answer_text(q_data.get('correct_answer', ''))
    q_data['options'] = [
        f'A) {correct}',
        f'B) alt{attempt_num}a',
        f'C) alt{attempt_num}b',
        f'D) alt{attempt_num}c'
    ]
```

**Pros**:
- Defense-in-depth: sanitize at source
- Protects against multiple downstream vulnerabilities
- Works with existing code

**Cons**:
- Adds a new function
- Requires testing sanitization against injection payloads

**Effort**: Medium (10 lines of code + tests)
**Risk**: Low (sanitization is additive, doesn't break existing logic)

### Solution B: Use Template Escaping Only
**Approach**: Rely on Jinja2/JavaScript template escaping to handle the sanitization.

**Pros**:
- No code changes needed
- Simpler implementation

**Cons**:
- Fragile: depends on downstream rendering
- Doesn't protect if JSON is parsed by JavaScript without escaping
- Violates security principle: "validate and sanitize at source"

**Effort**: Minimal (1 line: add config)
**Risk**: High (doesn't protect against all vectors)

### Solution C: Extract Answer Format Validation
**Approach**: Add validator rule to reject answers with dangerous characters.

**Code**:
```python
# In question_validator.py, add Rule 18
DANGEROUS_PATTERNS = [
    r'<', r'>', r'"', r"'", r'onclick', r'onerror', r'onload',
    r'javascript:', r'eval\(', r'<script'
]

def _check_dangerous_content(answer):
    """Rule 18: Answer must not contain potentially dangerous characters."""
    answer_lower = answer.lower()
    for pattern in DANGEROUS_PATTERNS:
        if pattern in answer_lower:
            return False, f'Answer contains dangerous content: {pattern}'
    return True, ''
```

**Pros**:
- Catches malicious input early
- Prevents bad answers from reaching students

**Cons**:
- Only catches known patterns (not comprehensive)
- Might reject legitimate LaTeX expressions

**Effort**: Medium (5+ lines in validator)
**Risk**: Medium (could falsely reject valid math expressions)

## Recommended Action

Implement **Solution A** — sanitize at the point where untrusted content is embedded.

### Step 1: Add Sanitization Function
Create new file or add to existing utils:
```python
# In ai/utils.py or a new ai/sanitizers.py

def sanitize_answer_text(text):
    """Escape special characters in answer text for safe embedding in options."""
    if not text:
        return ''
    text = str(text)
    # Remove control characters
    text = ''.join(c for c in text if ord(c) >= 32 or c in '\n\t')
    # Escape HTML special characters
    text = (text.replace('&', '&amp;')
            .replace('<', '&lt;')
            .replace('>', '&gt;')
            .replace('"', '&quot;')
            .replace("'", '&#x27;')
            .replace('\n', ' ').replace('\r', ' '))
    return text
```

### Step 2: Update Placeholder Generation
Modify line 263 in `services/question_service.py`:
```python
if q_type == 'mcq' and q_data and not q_data.get('options'):
    correct = sanitize_answer_text(q_data.get('correct_answer', ''))  # Sanitized
    q_data['options'] = [
        f'A) {correct}',
        f'B) alt{attempt_num}a',
        f'C) alt{attempt_num}b',
        f'D) alt{attempt_num}c'
    ]
```

### Step 3: Add Security Tests
```python
def test_xss_payload_sanitized():
    """Verify XSS payloads are sanitized in placeholder options."""
    payloads = [
        '5" onclick="alert(1)"',
        '<img src=x onerror=alert(1)>',
        'javascript:alert(1)',
        '"><script>alert(1)</script>',
    ]
    for payload in payloads:
        result = sanitize_answer_text(payload)
        assert '<' not in result
        assert '>' not in result
        assert 'onclick' not in result
        assert 'javascript' not in result
```

### Step 4: Verify No Regressions
Run full test suite:
```bash
python3 -m pytest tests/ -v
```

## Acceptance Criteria

- [ ] Sanitization function defined and tested
- [ ] Placeholder options use sanitized answer text
- [ ] XSS payloads are neutralized (tested with OWASP examples)
- [ ] No breaking changes to question generation
- [ ] Performance impact negligible (<1ms per question)

## Technical Details

### Affected Files
- `services/question_service.py` (line 263)
- New file: `ai/sanitizers.py` (if creating new module)

### Database Impact
Sanitized text is stored in the database. No migration needed (backward compatible).

### Frontend Impact
Sanitized text is rendered safely in HTML/JavaScript contexts.

## Institutional Learnings

From past commits (c357469 "Harden question generation pipeline"), the codebase already has a pattern for this:
- `ai/json_utils.py`: Type-safe JSON parsing with error messages
- Defensive layers: exception handling, type checking, validation

This XSS fix follows the same pattern: add a sanitization layer at the data source.

## Work Log

- **2026-02-12**: Created todo after security audit. Security-Sentinel flagged XSS vulnerability in placeholder generation.

## Resources

- OWASP Top 10: A03:2021 Injection
- OWASP XSS Prevention Cheat Sheet: https://cheatsheetseries.owasp.org/cheatsheets/Cross_Site_Scripting_Prevention_Cheat_Sheet.html
- Related code: `ai/json_utils.py` (defensive parsing pattern)

---

**Status**: PENDING TRIAGE
**Priority**: P2 (IMPORTANT - Should Fix)
**Assigned to**: (none)
